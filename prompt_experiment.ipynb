{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d482896",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''\n",
    "You are an expert in converting natural language descriptions of events into structured annotations using a predefined Hierarchical Event Descriptor (HED) vocabulary. Your task is to extract relevant concepts from the input description and represent them as a comma-separated list of HED tags, strictly adhering to the provided vocabulary.\n",
    "\n",
    "**Crucial Constraints:**\n",
    "* **Vocabulary Adherence:** You MUST ONLY use tags (single terms) found in the provided HED vocabulary schema. Do not introduce any new words, synonyms, or full hierarchical paths in the annotation. When a concept maps to a hierarchical tag, use only the **most specific (leaf) tag** from the hierarchy.\n",
    "* **Output Format:** Your output must be a comma-separated list of HED tags. Each tag or nested group of tags should be enclosed in parentheses `()`. For tags that take a value (indicated by `#` in the schema), replace `#` with the appropriate value from the description.\n",
    "* **Completeness:** Capture all relevant information from the input description using the vocabulary.\n",
    "* **Conciseness:** Avoid redundant tags.\n",
    "* **No Explanations:** Do not provide any conversational filler, explanations, or additional text beyond the annotations themselves, *except for the reasoning process requested below*.\n",
    "* **Handling Unmappable Concepts:** If a concept in the description cannot be mapped to any tag in the vocabulary, omit it. Do not invent tags or try to approximate.\n",
    "\n",
    "**Reasoning Process (Output this first, then the Annotation):**\n",
    "Before providing the final annotation, detail your thought process in the following steps:\n",
    "1.  **Breakdown:** Identify the key elements or phrases in the description that represent distinct concepts.\n",
    "2.  **Mapping:** For each key element, find the most specific corresponding tag in the provided HED vocabulary schema. If a concept cannot be mapped, note it as unmappable.\n",
    "3.  **Structuring:** Determine how the identified tags should be grouped or nested according to HED conventions (e.g., (Object, Action, Object) or (Main_Event, (Property, Value))).\n",
    "\n",
    "**HED Vocabulary Schema:**\n",
    "{{hed_vocab}}\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "Description: The foreground view consists of a large number of ingestible objects, indicating a high quantity. The background view includes an adult human body, outdoors in a setting that includes furnishings, natural features such as the sky, and man-made objects in an urban environment\n",
    "\n",
    "--- REASONING PROCESS START ---\n",
    "1.  **Breakdown**: The description includes elements about visual perspective (foreground/background), quantity, types of objects, human characteristics, environmental setting, and urban features.\n",
    "2.  **Mapping**:\n",
    "    * \"foreground view\": `Foreground-view`\n",
    "    * \"large number\": `Item-count`, `High`\n",
    "    * \"ingestible objects\": `Ingestible-object`\n",
    "    * \"background view\": `Background-view`\n",
    "    * \"adult human body\": `Human`, `Body`\n",
    "    * \"outdoors\": `Outdoors`\n",
    "    * \"furnishings\": `Furnishing`\n",
    "    * \"natural features\": `Natural-feature` (since \"sky\" isn't a leaf tag, use parent)\n",
    "    * \"man-made objects\": `Man-made-object`\n",
    "    * \"urban environment\": `Urban`\n",
    "3.  **Structuring**: Group related visual elements, quantify, specify human attributes, and categorize environmental aspects.\n",
    "--- REASONING PROCESS END ---\n",
    "\n",
    "--- ANNOTATION START ---\n",
    "(Foreground-view, (Item-count, High), Ingestible-object), (Background-view, (Human, Body, Outdoors, Furnishing, Natural-feature, Urban, Man-made-object)\n",
    "--- ANNOTATION END ---\n",
    "\n",
    "Description from user:\n",
    "{{description}}\n",
    "\n",
    "--- REASONING PROCESS START ---\n",
    "--- REASONING PROCESS END ---\n",
    "\n",
    "--- ANNOTATION START ---\n",
    "--- ANNOTATION END ---\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cfdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's tackle this annotation task. The user provided a description: \"A participant is looking at a computer screen displaying an image of a red car.\" I need to break this down using the given tags.\n",
      "\n",
      "First, I'll identify the key elements. The main components here are the participant, the computer screen, and the red car. Let's start with the participant. The tag for the participant's action might be under Task-property or maybe under Task-event-role. Wait, looking back at the tags, there's a Task-event-role with \"Participant-response\" and \"Task-activity\". But the participant here is the one observing, so maybe \"Participant-response\" isn't the right fit. Alternatively, maybe it's under the Task-action-type, like \"Appropriate-action\" or something else. Hmm, maybe not. Let me check the tags again.\n",
      "\n",
      "Wait, the tags include \"Task-event-role\" with \"Participant-response\" and \"Task-activity\". The participant's action here is observing, so maybe \"Task-activity\" could be relevant. But the main focus is the visual elements. Let's focus on the objects mentioned.\n",
      "\n",
      "The computer screen: in the tags, there's a \"Visual-presentation\" under Sensory-presentation, which includes \"2D-view\" and \"3D-view\". But the screen is a 2D display. Also, \"Background-view\" and \"Foreground-view\" are part of Visual-presentation. The screen itself might be part of the background or foreground. Wait, the participant is looking at the screen, so the screen is the main visual element. But the tags don't have a direct tag for a computer screen. However, the screen is part of the visual presentation. Maybe \"Visual-presentation\" with \"2D-view\" since it's a screen display.\n",
      "\n",
      "The red car: the color is red, so under \"Color\" in the Sensory-attribute. The car is an object, so \"Ingestible-object\"? No, that's for food. Wait, the tags have \"Object\" under Task-property? Let me check. Looking back, under Task-property, there's \"Task-stimulus-role\" with \"Target\", \"Distractor\", etc. But the car here is the main image, so maybe \"Target\" as the stimulus role. Also, the color red would be under \"Color\" in the Sensory-attribute. The car is a \"Man-made-object\" as well.\n",
      "\n",
      "Putting this together: The participant is looking at the screen, which is a 2D view. The car is a man-made object with a red color. So the annotations would include \"Visual-presentation\" with \"2D-view\", \"Man-made-object\", and \"Color\" with \"Red\". Also, the participant's action might be under \"Task-action-type\" as \"Appropriate-action\" or \"Participant-response\", but I need to check the tags again. Wait, \"Task-event-role\" includes \"Participant-response\", which might be the action here. Alternatively, the screen could be part of the \"Visual-presentation\" as the foreground.\n",
      "\n",
      "Wait, the original example had \"Foreground-view\" and \"Background-view\". In this case, the screen is the main visual, so maybe it's the foreground. But the screen itself isn't a view; it's the display. Hmm, maybe the screen is part of the background or foreground. Alternatively, the car is in the foreground. But the description says the participant is looking at the screen, which displays the car. So the screen is part of the visual setup. Maybe the screen is considered part of the background, but the car is in the foreground. Wait, but the screen is the medium, not the actual view. This is a bit confusing. Let me re-examine the tags.\n",
      "\n",
      "Looking at the tags, under Sensory-presentation, there's \"Visual-presentation\" with \"2D-view\" and \"3D-view\". The screen is a 2D display, so that's a key tag. The car is an object, so \"Man-made-object\", and the color red would be under \"Color\" in the Sensory-attribute. The participant's action might be under \"Task-event-role\" as \"Participant-response\" since they are looking at it. Alternatively, \"Task-action-type\" could have \"Appropriate-action\" if the task involves interacting with the screen. But the description doesn't specify a task action, just the observation. \n",
      "\n",
      "So, the main annotations would be: the visual presentation is 2D, the car is a man-made object with red color. The participant is looking at it, which might be under \"Participant-response\" in Task-event-role. But I need to make sure. Let me structure it step by step.\n",
      "\n",
      "1. \"A participant is looking at a computer screen\" – this is the participant's action. Under Task-event-role, \"Participant-response\" might fit here.\n",
      "2. \"displaying an image of a red car\" – the screen displays the car, which is a man-made object with red color. \n",
      "\n",
      "So the annotations would include \"Participant-response\" for the participant's action, \"Visual-presentation\" with \"2D-view\" for the screen, \"Man-made-object\" for the car, and \"Color\" with \"Red\" for the car's color. \n",
      "\n",
      "Wait, the screen itself might not be an object but the medium. However, the tags don't have a specific tag for a computer screen. So maybe the screen is part of the \"Visual-presentation\" as a 2D display. Alternatively, the car is the main object. \n",
      "\n",
      "I think the key elements are the car as a man-made object with red color, and the visual presentation being 2D. The participant's action is looking at it, which could be part of the task activity. But without more context on the task, maybe focusing on the visual elements is better. \n",
      "\n",
      "So the final annotation would be: (Visual-presentation, (2D-view)), (Man-made-object, (Color, Red)). And maybe the participant's action as (Participant-response) if that's applicable. But the user's example didn't include the participant, so maybe just the objects. \n",
      "\n",
      "Alternatively, the screen could be part of the environment, but the tags don't have a direct tag for that. So I'll focus on the car and the screen's display type.\n",
      "</think>\n",
      "\n",
      "--- REASONING PROCESS START ---\n",
      "1. **Breakdown**: The description includes a participant observing a computer screen displaying a red car. Key elements are the visual medium (screen), the object (car), and its attributes (color).\n",
      "2. **Mapping**:\n",
      "   - \"computer screen displaying an image\": `Visual-presentation` (2D-view) as the screen is a 2D display.\n",
      "   - \"red car\": `Man-made-object` (car) with `Color` (Red).\n",
      "   - \"participant looking at\": `Participant-response` (Task-event-role) to indicate the participant's action of observation.\n",
      "3. **Structuring**: Focus on the visual medium, object, and its attributes, while noting the participant's engagement.\n",
      "--- REASONING PROCESS END ---\n",
      "\n",
      "--- ANNOTATION START ---\n",
      "(Visual-presentation, (2D-view)), (Man-made-object, (Color, Red)), (Participant-response)\n",
      "--- ANNOTATION END ---\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "from google import genai\n",
    "\n",
    "from jinja2 import Template\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "# load the HED vocabulary from HED_vocab_reformatted.xml into a string\n",
    "hed_vocab = ''\n",
    "with open('HED_vocab_reformatted.xml', 'r') as file:\n",
    "    hed_vocab = file.read()\n",
    "\n",
    "# render the jinja template from prompt_template variable\n",
    "template = Template(prompt_template)\n",
    "\n",
    "query = 'A participant is looking at a computer screen displaying an image of a red car.'\n",
    "\n",
    "prompt = template.render(hed_vocab=hed_vocab, description=query)\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()\n",
    "\n",
    "model = 'qwen3:8b'\n",
    "# call the chat function with the model and the prompt\n",
    "response: ChatResponse = chat(model=model, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': prompt,\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "\n",
    "# save the prompt template and model response to a json file\n",
    "import json\n",
    "i = 0\n",
    "filename = 'prompt_experiments/experiment_'\n",
    "while os.path.exists(f'{filename}{i}.json'):\n",
    "    i += 1\n",
    "\n",
    "with open(f'{filename}{i}.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'model': model,\n",
    "        'prompt_template': prompt_template,\n",
    "        'model_response': response['message']['content']\n",
    "    }, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ffa86d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HED",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
